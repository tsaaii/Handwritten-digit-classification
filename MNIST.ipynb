{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = \"dr/train.csv\"\n",
    "test_file = \"dr/test.csv\"\n",
    "output_file = \"submission_dl.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt(train_file, skiprows=1, dtype='int', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " raw_data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 7, 6, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    raw_data[:,1:], raw_data[:,0], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_val = x_val.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\")/255.\n",
    "x_val = x_val.astype(\"float32\")/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n",
    "                 input_shape = (28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zoom_range = 0.1,\n",
    "                            height_shift_range = 0.1,\n",
    "                            width_shift_range = 0.1,\n",
    "                            rotation_range = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "150s - loss: 0.2568 - acc: 0.9429 - val_loss: 0.0601 - val_acc: 0.9800\n",
      "Epoch 2/30\n",
      "164s - loss: 0.2168 - acc: 0.9494 - val_loss: 0.0733 - val_acc: 0.9725\n",
      "Epoch 3/30\n",
      "150s - loss: 0.1566 - acc: 0.9627 - val_loss: 0.0614 - val_acc: 0.9850\n",
      "Epoch 4/30\n",
      "150s - loss: 0.1494 - acc: 0.9624 - val_loss: 0.0304 - val_acc: 0.9875\n",
      "Epoch 5/30\n",
      "152s - loss: 0.1281 - acc: 0.9662 - val_loss: 0.0250 - val_acc: 0.9925\n",
      "Epoch 6/30\n",
      "148s - loss: 0.1310 - acc: 0.9667 - val_loss: 0.0212 - val_acc: 0.9875\n",
      "Epoch 7/30\n",
      "147s - loss: 0.0937 - acc: 0.9756 - val_loss: 0.0208 - val_acc: 0.9950\n",
      "Epoch 8/30\n",
      "170s - loss: 0.1100 - acc: 0.9721 - val_loss: 0.0255 - val_acc: 0.9925\n",
      "Epoch 9/30\n",
      "221s - loss: 0.0965 - acc: 0.9739 - val_loss: 0.0272 - val_acc: 0.9875\n",
      "Epoch 10/30\n",
      "217s - loss: 0.0939 - acc: 0.9752 - val_loss: 0.0264 - val_acc: 0.9900\n",
      "Epoch 11/30\n",
      "206s - loss: 0.0759 - acc: 0.9784 - val_loss: 0.0238 - val_acc: 0.9950\n",
      "Epoch 12/30\n",
      "180s - loss: 0.0826 - acc: 0.9786 - val_loss: 0.0229 - val_acc: 0.9950\n",
      "Epoch 13/30\n",
      "211s - loss: 0.0812 - acc: 0.9777 - val_loss: 0.0144 - val_acc: 0.9950\n",
      "Epoch 14/30\n",
      "179s - loss: 0.0836 - acc: 0.9787 - val_loss: 0.0166 - val_acc: 0.9975\n",
      "Epoch 15/30\n",
      "178s - loss: 0.0791 - acc: 0.9789 - val_loss: 0.0155 - val_acc: 0.9950\n",
      "Epoch 16/30\n",
      "176s - loss: 0.0769 - acc: 0.9798 - val_loss: 0.0171 - val_acc: 0.9950\n",
      "Epoch 17/30\n",
      "204s - loss: 0.0736 - acc: 0.9795 - val_loss: 0.0187 - val_acc: 0.9950\n",
      "Epoch 18/30\n",
      "248s - loss: 0.0768 - acc: 0.9804 - val_loss: 0.0170 - val_acc: 0.9950\n",
      "Epoch 19/30\n",
      "233s - loss: 0.0716 - acc: 0.9806 - val_loss: 0.0180 - val_acc: 0.9900\n",
      "Epoch 20/30\n",
      "251s - loss: 0.0575 - acc: 0.9830 - val_loss: 0.0138 - val_acc: 0.9975\n",
      "Epoch 21/30\n",
      "232s - loss: 0.0583 - acc: 0.9830 - val_loss: 0.0114 - val_acc: 0.9975\n",
      "Epoch 22/30\n",
      "203s - loss: 0.0691 - acc: 0.9824 - val_loss: 0.0138 - val_acc: 0.9975\n",
      "Epoch 23/30\n",
      "224s - loss: 0.0634 - acc: 0.9821 - val_loss: 0.0153 - val_acc: 0.9950\n",
      "Epoch 24/30\n",
      "221s - loss: 0.0550 - acc: 0.9846 - val_loss: 0.0135 - val_acc: 0.9950\n",
      "Epoch 25/30\n",
      "216s - loss: 0.0605 - acc: 0.9845 - val_loss: 0.0167 - val_acc: 0.9925\n",
      "Epoch 26/30\n",
      "177s - loss: 0.0543 - acc: 0.9852 - val_loss: 0.0159 - val_acc: 0.9950\n",
      "Epoch 27/30\n",
      "217s - loss: 0.0558 - acc: 0.9840 - val_loss: 0.0138 - val_acc: 0.9950\n",
      "Epoch 28/30\n",
      "196s - loss: 0.0542 - acc: 0.9857 - val_loss: 0.0154 - val_acc: 0.9950\n",
      "Epoch 29/30\n",
      "212s - loss: 0.0521 - acc: 0.9854 - val_loss: 0.0160 - val_acc: 0.9950\n",
      "Epoch 30/30\n",
      "198s - loss: 0.0555 - acc: 0.9863 - val_loss: 0.0152 - val_acc: 0.9950\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=16),\n",
    "                           steps_per_epoch=700,\n",
    "                           epochs=30, #Increase this when not on Kaggle kernel\n",
    "                           verbose=2,  #1 for ETA, 0 for silent\n",
    "                           validation_data=(x_val[:400,:], y_val[:400,:]), #For speed\n",
    "                           callbacks=[annealer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.0223, final accuracy: 0.9924\n"
     ]
    }
   ],
   "source": [
    "final_loss, final_acc = model.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_testset = np.loadtxt(test_file, skiprows=1, dtype='int', delimiter=',')\n",
    "x_test = mnist_testset.astype(\"float32\")\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_test, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_hat,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f :\n",
    "    f.write('ImageId,Label\\n')\n",
    "    for i in range(len(y_pred)) :\n",
    "        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
